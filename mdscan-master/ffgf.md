---
title: "Model Evaluation and Validation "
author: "Gabriel Kallah-Dagadu"
date: "2024-07-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Model evaluation and validation
The ultimate purpose of spatial predictive modelling is to generate spatial predictions for a target area by developing and applying an accurate predictive model. The predictive accuracy of the model is critical, as it determines the quality of the resulted predictions that form the base for further scientific activities and/or decision-making. Therefore, how to correctly evaluate the accuracy of predictive models is vitally important. This session introduces various accuracy and error measures that are used to conduct the evaluation of predictive models for numerical and categorical data. It will also introduce various model validation methods, and techniques to handle randomness associated with cross-validation methods.

### Predictive errors, observational errors, and true predictive errors
Predictive accuracy is a measure to assess how accurate are the predictions generated by a predictive model. For a dependent variable, the predictions should be as close as possible to the true values of the variable. Prior to assessing the accuracy, we need to introduce the concepts of observed and predicted values.

*Observed values and predicted values*
_Observed values_ are the values of validation samples that are to be used to assess the performance of a predictive model. Since there are errors inevitably associated with the process acquiring observed values, observed values are often not true values. 
Thus for a given observed value, it may be different from its corresponding true value. The difference between true value $(\tau_i)$ and observed value $(ùë¶_i)$, $\tau_i- y_i$, is the error associated with the observed value, which is referred to as _observational error_ $(\epsilon_{oi})$.
The _observational error_, $\epsilon_{oi}$ is the sum of random error associated with observed dependent variable, and sampling and measuring errors. 

The _sampling_ and _measuring errors_ result from various factors, including sampling design, the position accuracy of survey vessel, equipment used for sample collection, field operation, sample storage, sample processing procedure and analysis in laboratory, and data entry, that may affect the accuracy of observation and change with the variable observed.
However, how much error can each of these factors contribute to the sampling and measuring errors is unknown in most cases. Since the true values are often unknown, we have to use observed values to assess the predictive accuracy in practice, but these errors should be taken into account when using the predictive accuracy to assess the quality of spatial predictions resulted, where professional knowledge plays its roles.

*Relationships of predictive error with observational error and true predictive error*

Theoretically, the _predictive error_ should be the difference between the _predicted value_ $(\hat y_i)$ 
for and the true value of a non-training sample, which we refer to as _true predictive error_ $(\epsilon_{tpi})$.

The _predictive error_ $(\epsilon_{pi})$ can be represented in terms of the _observational error_ $(\epsilon_{oi})$ and the _true predictive error_ $(\epsilon_{tpi})$ as in Equation below:

$$
\epsilon_{pi}= y_i-\hat y_i\\
 =(\tau_i-\hat y_i)-(\tau_i-y_i)\\
=\epsilon_{tpi}-\epsilon_{oi}
$$
To assess the predictive accuracy, we need to further explore the relationships of predictive error with observational error and true predictive error in relation to observed values, predicted values, and true values, so that we can have a better understanding of the predictive accuracy derived.

For some model performance measures such as *mean absolute error* (MAE) and *mean square error* (MSE), the *absolute predictive error*, the absolute value of the difference between predicted values and observed values, is used either explicitly or implicitly, that is,
$\epsilon_{api}= |y_i-\hat y_i|.$

## Accuracy and error measures for numerical data
On the basis of predicted and observed values, many accuracy and error measures have been developed to assess the accuracy of predictive models for numerical data.

Some of these measures are provided in Table 3.2 of Spatial Predictive Modeling with R (Li, 2022) based on observed values ($y$, a vector of $n$ values) and predicted values ($\hat y$, a vector of $n$ values). These measures include accuracy measures such as _variance explained_ (VEcv) (Li 2016) and _Legates and McCabe‚Äôs efficiency_ (ùê∏1) (Legates & McCabe 2013), as well as the most commonly used error measures such
as _MAE_ and _root mean square error_ (RMSE) (Li & Heap 2008).
All measures are related to either $\epsilon_{pi}$ or $\epsilon_{api}$. Of these measures, _mean error_ (ME) and _relative mean error_ (RME)
are derived from $\epsilon_{pi}$, while _Mean absolute error_ (MAE), _Relative MAE_ (RMAE), _Mean square error_ (MSE), _Root MSE_ (RMSE), _relative RMSE_ (RRMSE), _Standardized RMSE_ (SRMSE), and _Mean square reduced error_ MSRE are
derived from $\epsilon_{api}$. 

The _mean standardized error_ (MSE2) and _Root mean square standardized error_ (RMSSE) are also based on $\epsilon_{pi}$ and $\epsilon_{api}$, respectively, where
the ùëù$\epsilon_{pi}$ and $\epsilon_{api}$ are however derived from standardized predicted values and standardized observed values. 

## Accuracy and error measures for categorical data
For categorical data including presence/absence data, various accuracy measures have been developed. To understand these measures, we need to introduce an error matrix for two-level categorical data as example below.

An error matrix for observed values ($ùë¶$, a vector of $ùëõ$ values, i.e., $y_1, ùë¶_2,\cdots, ùë¶_ùëõ$) and predicted values ($\hat y$, a vector of $n$ values, i.e., $\hat y_1,\hat y_2, \cdots, \hat y_n$ ) for two-class categorical data (e.g., class A/class B).

```{r echo=FALSE, out.width="50%"}

knitr::include_graphics("pict1.png")
```

Measures for assessing the performance of predictive models for categorical data are listed below.

```{r echo=FALSE, out.width="50%"}

knitr::include_graphics("pict2.png")
```

Although these measures listed above are defined based on categorical data with two classes, they can be extended for data with multiple classes.

### R functions for accuracy and error measures
Accuracy and error measures for numerical and categorical data can be calculated in R. The recommended measures and some other commonly used measures are implemented in the function `pred.acc` in the `spm` package. A further function, `vecv`, is also developed for the accuracy measure `VEcv` in the `spm` package. Since some error measures can be converted into `VEcv` (Li 2016), a function, `tovecv`, is also developed in the `spm` package for such conversion.
The description, usage, arguments, and returned values of `pred.acc`, `vecv`, and `tovecv` are provided below:

```{r, error=FALSE}
library(spm)
#Implementation of `pred.acc` for numerical data
set.seed (1234)
y <- sample (1:50, 200, replace = T)
yhat <- y + rnorm(200, 1)
pred.acc1 <- pred.acc(y, yhat)
lapply(pred.acc1 , round , 2)
```

```{r}
round(pred.acc1$vecv , 2)#individual measure in pred.acc1 can be retrieved.
```
```{r}
#Implementation of pred.acc for categorical data (two classes)

set.seed (1234)
y2 <- as.factor(sample(c("A", "B"), 30, replace = T))
y2hat <- y2
y2hat[1] <- c("A"); y2hat [10] <- c("A"); y2hat [25] <- c("A"); y2hat[5] <- c("B");
y2hat [27] <- c("B")
table(y2, y2hat) # an error matrix
```
```{r}
pred.acc2 <- pred.acc(y2, y2hat)
lapply(pred.acc2 , round , 2)
#round(pred.acc2$ccr , 2)
```
```{r}
#Data with multiple classes
set.seed (1234)
y3 <- as.factor(sample(c("A", "B", "C"), 30, replace = T))
y3hat <- y3
y3hat[1] <- c("A"); y3hat [10] <- c("C"); y3hat [25] <- c("C"); y3hat[5] <- c("B");
y3hat [30] <- c("B")
table(y3, y3hat) # an error matrix
```
```{r}
pred.acc3 <- pred.acc(y3, y3hat)
lapply(pred.acc3 , round , 2)
```
### Model validation
The accuracy of predictive models is critical, as it determines the quality of predictions resulted. For a given data set, the accuracy is often assessed based on model validation methods. This subsection discusses methods of model validation. 

1. *Hold-out validation*
The _hold-out validation_ splits up a data set into a training sub-data set (to be used to develop a predictive model) and a validation sub-data set (to be used to test how well the model performs on unseen data). The size of the training sub-data sets varies depending on sample size, usually with around `50%` to `90%` of data for training and the remaining samples for validation.

2. *K-fold cross-validation*
The _ùëò-fold cross-validation_ randomly splits up a data set (or a stratified data set) into `ùëò groups`. One of the groups is to be used as a validation sub-data set and the remaining `ùëò ‚àí 1` groups are to be used as a training sub-data set. A predictive model is developed
based on the training sub-data set, and then it is used to make predictions based on the validation sub-data set. The process is repeated until each of the `ùëò` groups has been used as the validation sub-data set. Then the model performance is assessed based on the predicted values for and the observed values of the data set.
One may assess the model performance based on the predicted values for and the observed values of each validation sub-data set and then average relevant performance measure(s) over `ùëò-folds` to produce the final model performance measure(s). The results would be identical if the `ùëò groups` are equal in size.

3. *Leave-one-out and leave-q-out cross-validation*
_Leave-one-out_ (LOO) cross-validation is a special case of `ùëò-fold` cross-validation where the number of folds equals the number of the observations in the data set.

_Leave-ùëû-out_ cross-validation is similar to `LOO`, but it uses `ùëû samples` as the validation subdata set instead of just one sample.

4. *Bootstrapping cross-validation*
The _bootstrapping cross-validation_ splits up a data set into 
(1) a training sub-data set by sampling the data set with replacement (i.e., with the training sub-data set resulted containing about two-thirds of samples of the data set and one-third duplicated samples),and 
(2) a validation sub-data set that is remaining samples left out of the bootstrapped training sub-data set. This process may repeat many times. This method can not be used for geostatistical methods such as inverse distance weighted (IDW) due to duplicated samples with the zero distance that cannot be used as a denominator.

### Validation functions in R
Many R functions for evaluating the predictive accuracy of relevant predictive methods based on cross-validation have been developed. Most of these functions are available in the `spm` and `spm2` packages and some are scattered in other packages.
In environmental sciences, the most commonly used validation methods are hold-out and
leave-one-out (Li 2016, 2019a), although five- or 10-fold cross-validation is recommended (Kohavi 1995; Hastie, Tibshirani, and Friedman 2009). 
We will demonstrate the applications of these three validation methods.

The functions `krigepred` and `krigecv` in the `spm2` package and the `swmud` data set in the `spm` package will be used for the demonstration.
For `krigepred`, the following arguments may need to be specified:
(1) _trainx_, a dataframe contains longitude (long) and latitude (lat) and predictive variables of point samples;
(2) _trainy_, a vector of response, must have length equal to the number of rows in _trainx_
(3) _trainx2_, a dataframe contains longitude (long), latitude (lat) and predictive variables of point locations (i.e., the centers of grids) to be predicted;
(4) _nmax_, the number of nearest observations that should be used for a prediction; and
(5) _vgm.args_, arguments for `vgm`, e.g., variogram model of response variable and anisotropy parameters and see `vgm` in the `gstat` package for details; and by default, `Sph` is used.

1. *Hold-out validation: Random sampling*
```{r}
library(spm)
data(swmud)
set.seed (1234)
tr <- sample (1:dim(swmud)[1], floor(dim(swmud)[1]*0.9))
training1 <- swmud[tr, ]
validation1 <- swmud[-tr, ]
```

We then use `krigepred` function to generate predictions using *ordinary kriging* (OK) for the validation samples.
```{r}
library(spm2)
okpred1 <- krigepred(training1[, c(1,2)], training1[,3], validation1 , nmax = 12, vgm.args = ("Sph"))
names(okpred1)
```
The predictive accuracy of OK (i.e., okpred1) can be evaluated using `vecv` function from `spm` package.
```{r}
okvecvho1 <- vecv(validation1$mud , okpred1$var1.pred)
round(okvecvho1 , 2)
```
*Stratified random sampling*
The function `datasplit` in the `spm2` package, which implements a stratified random resampling technique, can be used to split a data set into a training sub-data set (90% observations) and a validation sub-data set (10% observations).

```{r}
library(spm2)
set.seed (1234)
idx1 <- datasplit(swmud[, 3], k.fold = 10)
training2 <- swmud[idx1 != c(1), , drop = FALSE]
validation2 <- swmud[idx1 == c(1), , drop = FALSE]
```

Then `OK` can be employed to generate spatial predictions for the validation data set, and `vecv` can be used to assess the predictive accuracy of the predictions resulted.
```{r}
okpred2 <- krigepred(training2[, c(1,2)], training2[,3], validation2 , nmax = 12, vgm.args = ("Sph"))
names(okpred2)
okvecvho2 <- vecv(validation2$mud , okpred2$var1.pred)
round(okvecvho2 , 2)
```
The predictive accuracy for random sampling is lower than that for stratified random sampling. The difference in the predictive accuracy could be due to: 
(1) stratified random sampling method that is expected to produce a higher accuracy than the random sampling method, and 
(2) randomness associated with the sampling methods that can be confirmed using some method discussed below.

2. *Leave-one-out cross-validation*
The function `krigecv` will be used to perform *LOO* cross-validation for OK based on the `swmud` data set.
```{r, warning=FALSE}
set.seed (1234)
okloo1 <- krigecv(swmud[, c(1,2)], swmud[, 3], validation = "LOO", nmax = 12, vgm.args = ("Sph"), predacc = "VEcv")
round(okloo1 , 2)
```

3. *10-fold cross-validation*
The function `krigecv` will also be used to perform 10-fold cross-validation for `OK` based on the swmud data set.

```{r warning=FALSE}
ok10f <- krigecv(swmud[, c(1,2)], swmud[, 3], validation = "CV", cv.fold = 10, nmax = 12, vgm.args = ("Sph"), predacc = "VEcv")
round(ok10f , 2)
```

### Effects of randomness associated of cross-validation methods on predictive accuracy assessments

Although five- or 10-fold cross-validation is recommended to evaluate the performance of predictive models (Kohavi 1995), the training and validation data sets that are randomly generated for each fold of the cross-validation change when the process is repeated. Consequently, the predictive accuracy or error measures resulted also change with each iteration of the cross-validation and are not stable (Li 2013c). 
Thus the randomness associated with the cross-validation would affect the predictive accuracy assessments.

To reduce the influence of the randomness on predictive accuracy assessments, we need to stabilize the performance measures resulted by repeating the cross-validation a certain times (e.g., 100 times) (Li 2013c, 2013b; Li et al., 2014). The stabilization of the predictive model performance measures is to be demonstrated using OK with 100 repetitions of 10-fold cross-validation based on the swmud data set.

Prior to stabilizing the predictive accuracy, we will demonstrate: 
(1) the dependence of predictive accuracy or error measures on random seeds; and 
(2) the dependence of averaged predictive model performance measures on random seeds.

1. *Dependence of predictive accuracy measures on random seeds*
First, we generate 100 random seeds.
```{r}
set.seed (1234)
random.seed <- sample (1:9999, 100)
```

Then, we use each of the random seeds for *OK* by applying `krigecv` function to the swmud data set.
```{r warning=FALSE}
random.seed.vecv <- NULL
for (i in 1:length(random.seed)) {
set.seed(random.seed[i])
okcv1 <- krigecv(swmud[, c(1, 2)], swmud[, 3], nmax = 12, predacc = "VEcv")
random.seed.vecv[i] <- okcv1
}
```

The predictive accuracy resulted, `VEcv` in okcv1, can be visualized against random.seed

```{r warning=FALSE}
plot(random.seed.vecv ~ random.seed , xlab = "Random seed", ylab = "VEcv (%)", col
= "red")
```

It shows that the predictive accuracy changes considerably with the random seed used, ranging from 80.59% to 85.26%.
It is clear that the predictive accuracy resulted depends on random seeds used, and we should minimize the dependence on random seed in deriving relevant predictive accuracy assessments.

2. *Dependence of averaged predictive accuracy measures on random seeds*
To stabilize the predictive accuracy resulted by repeating the cross-validation a certain times, we need to determine the iteration number. The choice of iteration number is data dependent and can be determined based on the method used in (Li 2013c, 2013b, 2019a; Li et al. 2014); and 60 to 100 times are recommended (e.g., see various cross-validation functions in spm and spm2). 
However, the median and average accuracy resulted may depend on the random seed used. This can be proved as follows.

```{r}
#We generate ten random seeds first.
set.seed (1234)
r.seed <- sample (1:9999, 10)
```

```{r warning=FALSE}
#each of the random seeds for OK by applying krigecv to the swmud data set 100 times
n <- 100
r.seed.vecv <- matrix(0, n, length(r.seed))
for (i in 1:length(r.seed)) {
set.seed(r.seed[i])
for (j in 1:n) {
okcv1 <- krigecv(swmud[, c(1, 2)], swmud[, 3], nmax = 12, predacc = "VEcv")
r.seed.vecv[j, i] <- okcv1
}
}
```

The median and average predictive accuracy resulted, `VEcv` in `r.seed.vecv`, for the 10 random seeds are
```{r warning=FALSE}
library(miscTools)
colMedians(r.seed.vecv)
```
```{r}
colMeans(r.seed.vecv)
```

```{r}
range(colMedians(r.seed.vecv))
range(colMeans(r.seed.vecv))
```

It is apparent that the changes of median and average predictive accuracy based on 100 repetitions with random seeds are minimal, and the dependence of averaged predictive accuracy on the random seed is largely negligible. 
Hence, we can stabilize the predictive accuracy using any random seed for reproducible simulations.

3. *Stabilization of the accuracy of predictive model*
We use random seed `1234` to demonstrate how to stabilize the predictive accuracy of OK and produce reliable averaged accuracy by repeating the cross-validation `n` (e.g., 100) times.

```{r warning=FALSE}
n <- 100
VEcv <- NULL
for (i in 1:n) {
okcv1 <- krigecv(swmud[, c(1, 2)], swmud[, 3], nmax =12, predacc = "VEcv")
VEcv [i] <- okcv1
}
```

The median, mean, and range of the predictive accuracy (VEcv in VEcv) are
```{r}
round(median(VEcv), 2)#Median
round(mean(VEcv), 2) # Mean
round(range(VEcv), 2) #Range
```

The variation of `VEcv` and their accumulative median and average with each of the iterations are calculated using the functions `cummean` and `cummdedian` in the `cumstats` package.
```{r, warning=FALSE}
library(cumstats)
plot(VEcv ~ c(1:n), xlab = "Iteration for OK", ylab = "VEcv (%)")
points(cummean(VEcv) ~ c(1:n), col = "red")
points(cummedian(VEcv) ~ c(1:n), col = "blue")
abline(h = mean(VEcv), col = "red", lwd=2)
abline(h = median(VEcv), col = "blue", lwd=2)
```

The deviations of the accumulative median and average from the overall mean and median are illustrated as:
*maximal change of accumulative average (maxdeltacummean)*
```{r, warning=FALSE}
library(cumstats)
maxdeltacummean <- (range(VEcv)[2] - range(VEcv)[1])/(2*100)
plot((cummean(VEcv) - mean(VEcv)) ~ c(1:n), ylim = c(-0.25, 0.4), col = "red", xlab = "Iteration for OK", ylab = "Deviation in VEcv (%)")
points((cummedian(VEcv) - median(VEcv)) ~ c(1:n), col = "blue")
abline(h = 0, lwd = 1)
abline(h = maxdeltacummean , lty = 2)
abline(h = - maxdeltacummean , lty = 2)
abline(h = 0.1, lty = 2, col = "green")
abline(h = - 0.1, lty = 2, col = "green")
```

The deviations of the accumulative median and average from the overall mean and median, respectively, at each of the 100 iterations (black line), deviation of accumulative average (red circle), deviation of accumulative median (blue circle), the maximal change of accumulative average as threshold (black dashed lines) and 0.1% as threshold
(green dashed lines).

The accumulative median is quickly stabilized as the number of iteration increases and remains largely unchanged when the iteration number is over 70 if `maxdeltacummean` is used as a threshold. Similarly, if a different threshold (e.g., 0.1%) is used, the accumulative median will be stabilized at over 70 iterations.
The accumulative average is gradually stabilized as the number of iteration increases and remains largely stable when the iteration number is over 90 if `maxdeltacummean` is used as a threshold to obtain a stabilized predictive accuracy. If a different threshold (e.g., 0.1%) is used, then the accumulative average will be stabilized at around 50 iterations.

*With a pre-set threshold*
If a threshold is set prior to the modelling, then it can be used to stabilize the predictive accuracy. For instance, if the threshold is set as the change in overall median no more than 0.005%. Then we can use it to obtain a stabilized predictive accuracy as follows:
```{r, warning=FALSE}
threshold <- 0.005
set.seed (1234)
VEcv2 <- NULL
okcv1 <- krigecv(swmud[, c(1, 2)], swmud[, 3], nmax = 12, predacc = "VEcv")
VEcv2[1] <- okcv1
```

```{r, warning=FALSE}
vecvmedian1 <- median(VEcv2)
repeat {
okcv1 <- krigecv(swmud[, c(1, 2)], swmud[, 3], nmax = 12, predacc = "VEcv")
VEcv2[length(VEcv2) + 1] <- okcv1
vecvmedian2 <- median(VEcv2)
if (abs(vecvmedian2 - vecvmedian1) <= threshold) {
break
}
vecvmedian1 <- vecvmedian2
}
```

The number of iterations and the stabilized predictive accuracy in terms of `VEcv` are
```{r}
length(VEcv2)

round(vecvmedian2 , 2)
```


